{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346cb8ad",
   "metadata": {},
   "source": [
    "# Proyecto: Naive Bayes con Estimación KDE para Mantenimiento Predictivo\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Diseñar e implementar un clasificador propio basado en **Naive Bayes**, en el que la verosimilitud $P(x_i \\mid y)$ se estime utilizando técnicas de **Kernel Density Estimation (KDE)** en lugar de asumir una distribución normal.\n",
    "\n",
    "Se utilizará el **AI4I 2020 Predictive Maintenance Dataset**, con la variable objetivo `Machine failure`. El proyecto buscará responder empíricamente si reemplazar la suposición gaussiana clásica por KDE resulta en una mejora real en desempeño.\n",
    "\n",
    "---\n",
    "\n",
    "## Fundamento teórico\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "El clasificador **Naive Bayes** es un modelo probabilístico basado en el **teorema de Bayes** con una fuerte asunción de independencia entre las variables dado el valor de la clase. Su objetivo es calcular la probabilidad posterior de una clase $y$ dado un vector de atributos $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$:\n",
    "\n",
    "$$\n",
    "P(y \\mid \\mathbf{x}) = \\frac{P(y) \\cdot P(\\mathbf{x} \\mid y)}{P(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Dado que $P(\\mathbf{x})$ es constante para todas las clases en un problema de clasificación, la predicción se basa en:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_y \\; P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "La clave está en estimar adecuadamente las distribuciones de las verosimilitudes $P(x_i \\mid y)$.\n",
    "\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "\n",
    "En el enfoque **Gaussian Naive Bayes (GNB)**, se hace la **asunción paramétrica** de que cada variable numérica sigue una distribución **normal univariada** dentro de cada clase. Es decir:\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\mathcal{N}(x_i \\mid \\mu_{iy}, \\sigma^2_{iy}) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_{iy}}} \\exp\\left( -\\frac{(x_i - \\mu_{iy})^2}{2\\sigma^2_{iy}} \\right)\n",
    "$$\n",
    "\n",
    "Donde $\\mu_{iy}$ y $\\sigma^2_{iy}$ se estiman directamente desde los datos de entrenamiento.\n",
    "\n",
    "Este supuesto simplifica el modelo, pero puede ser problemático si los datos no siguen una distribución gaussiana, por ejemplo, si son **multimodales, sesgados o tienen colas pesadas**.\n",
    "\n",
    "\n",
    "### KDE como alternativa para estimar la verosimilitud\n",
    "\n",
    "Una alternativa interesante a asumir una distribución normal es usar un enfoque **no paramétrico** para estimar la densidad de probabilidad: la **Kernel Density Estimation (KDE)**.\n",
    "\n",
    "En este método, la verosimilitud $P(x_i \\mid y)$ se estima directamente a partir de los datos de entrenamiento, sumando pequeñas funciones kernel centradas en cada observación. La forma general es:\n",
    "\n",
    "$$\n",
    "\\hat{f}(x) = \\frac{1}{n h} \\sum_{i=1}^{n} K\\left( \\frac{x - x_i}{h} \\right)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $K$ es la función kernel (por ejemplo, gaussiana, triangular, rectangular),\n",
    "- $h$ es el parámetro de suavizado o **bandwidth**,\n",
    "- $x_i$ son las observaciones de la clase $y$.\n",
    "\n",
    "Este enfoque permite capturar formas de distribución **arbitrarias** y potencialmente más realistas que una gaussiana simple, lo cual podría mejorar la clasificación en contextos donde la suposición de normalidad no se cumple.\n",
    "\n",
    "En este proyecto, exploraremos esta idea usando tres variantes de KDE y comparándolas contra Gaussian Naive Bayes, evaluando cuál ofrece un mejor desempeño en el contexto de mantenimiento predictivo.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- **Nombre:** AI4I 2020 Predictive Maintenance Dataset  \n",
    "- **Fuente:** UCI / Kaggle  \n",
    "- **Target:** `Machine failure` (binaria)  \n",
    "- **Características:** Datos de sensores, condiciones operativas, tiempos de uso  \n",
    "- **Consideración crítica:** Dataset **desbalanceado**, con una minoría de fallas reales.  \n",
    "- Se recomienda realizar una revisión de la literatura sobre cómo se ha abordado el desbalance en este dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Metodología\n",
    "\n",
    "### 1. Implementación de clasificador Naive Bayes personalizado\n",
    "\n",
    "- Se implementará un clasificador Naive Bayes desde cero.\n",
    "- Para cada clase $y$, se estima la verosimilitud de cada variable continua $x_i$ como $P(x_i \\mid y)$ usando métodos no paramétricos de KDE.\n",
    "- Se comparará el rendimiento del modelo con un `GaussianNB` de `sklearn` como baseline.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Evaluación de tres métodos de KDE\n",
    "\n",
    "Cada grupo deberá comparar los siguientes tres métodos de estimación de densidad para calcular la verosimilitud $P(x_i \\mid y)$.\n",
    "\n",
    "\n",
    "\n",
    "#### a. KDE clásico con kernel gaussiano + optimización de `bandwidth`\n",
    "\n",
    "- Se utiliza un kernel gaussiano para estimar la densidad:\n",
    "  \n",
    "  $$\n",
    "  \\hat{f}(x) = \\frac{1}{n h \\sqrt{2\\pi}} \\sum_{i=1}^{n} \\exp\\left( -\\frac{(x - x_i)^2}{2h^2} \\right)\n",
    "  $$\n",
    "\n",
    "- El parámetro `bandwidth` $h$ controla la suavidad de la estimación.\n",
    "- Para seleccionar un valor apropiado de $h$, se debe usar un método de optimización como **Grid Search** o preferiblemente **optimización bayesiana**, evaluando el rendimiento del modelo en términos de **AUC ROC** mediante validación cruzada (al menos 5 folds).\n",
    "\n",
    "\n",
    "\n",
    "#### b. KDE tipo Parzen (ventanas simples)\n",
    "\n",
    "- En este enfoque, se reemplaza el kernel gaussiano por una ventana de forma simple, como rectangular o triangular.\n",
    "\n",
    "  - **Ventana rectangular (tophat):**\n",
    "    $$\n",
    "    K(u) = \\frac{1}{2} \\cdot \\mathbb{I}(|u| \\leq 1)\n",
    "    $$\n",
    "\n",
    "  - **Ventana triangular:**\n",
    "    $$\n",
    "    K(u) = (1 - |u|) \\cdot \\mathbb{I}(|u| \\leq 1)\n",
    "    $$\n",
    "\n",
    "- El modelo general de densidad estimada sigue siendo:\n",
    "\n",
    "  $$\n",
    "  \\hat{f}(x) = \\frac{1}{n h} \\sum_{i=1}^{n} K\\left( \\frac{x - x_i}{h} \\right)\n",
    "  $$\n",
    "\n",
    "- El ancho de ventana $h$ debe definirse manualmente y mantenerse fijo.\n",
    "- Puede implementarse directamente o usando `sklearn.neighbors.KernelDensity` con `kernel='tophat'` o `'linear'`.\n",
    "\n",
    "\n",
    "#### c. KDE con regla de Silverman\n",
    "\n",
    "- Se utiliza un kernel gaussiano, pero el `bandwidth` se calcula automáticamente con la **regla de Silverman**:\n",
    "\n",
    "  $$\n",
    "  h = 1.06 \\cdot \\hat{\\sigma} \\cdot n^{-1/5}\n",
    "  $$\n",
    "\n",
    "- La estimación de la densidad sigue la misma forma que el KDE gaussiano clásico:\n",
    "\n",
    "  $$\n",
    "  \\hat{f}(x) = \\frac{1}{n h \\sqrt{2\\pi}} \\sum_{i=1}^{n} \\exp\\left( -\\frac{(x - x_i)^2}{2h^2} \\right)\n",
    "  $$\n",
    "\n",
    "- Este método no requiere ajuste de hiperparámetros.\n",
    "- Se puede implementar fácilmente con `scipy.stats.gaussian_kde`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Instrucciones de evaluación\n",
    "\n",
    "- **Métrica principal:** Área bajo la curva ROC (AUC ROC)\n",
    "- **Procedimiento de validación:** Validación cruzada estratificada de 5 folds\n",
    "  - Usar el mismo esquema de partición para todos los métodos para asegurar comparabilidad.\n",
    "  - Calcular el AUC promedio y su desviación estándar para cada modelo.\n",
    "  - Registrar los tiempos de procesamiento\n",
    "\n",
    "- **Modelos a comparar:**\n",
    "  - Naive Bayes clásico (`GaussianNB`. Pueden usar el de Scikit Learn)\n",
    "  - Naive Bayes con KDE:\n",
    "    - KDE con kernel gaussiano y bandwidth ajustado por optimización\n",
    "    - KDE con ventana tipo Parzen\n",
    "    - KDE con regla de Silverman\n",
    "\n",
    "- **Objetivo del análisis:**\n",
    "  - Determinar si vale la pena reemplazar la suposición de normalidad por KDE en este problema.\n",
    "  - Comparar las tres variantes de KDE y explicar cuál se comporta mejor y bajo qué condiciones (tamaño del bandwidth, forma de la distribución, impacto del desbalance, etc.).\n",
    "  - Identificar si el ajuste del bandwidth influye significativamente en el desempeño del modelo correspondiente.\n",
    "  - Comparar el tiempo de computación de cada método y analizar su impacto en la viabilidad práctica.\n",
    "\n",
    "---\n",
    "\n",
    "## Consideraciones adicionales\n",
    "\n",
    "- **Desbalanceo:**\n",
    "  - Investigar y aplicar estrategias adecuadas para el tratamiento del desbalance en el dataset (por ejemplo: submuestreo, sobreponderación, uso de métricas balanceadas).\n",
    "  - Buscar artículos académicos (Google Scholar, Scopus, etc.) que hayan trabajado con el AI4I 2020 dataset y reportado técnicas o resultados relevantes para manejar el desbalanceo.\n",
    "  \n",
    "- **Preprocesamiento de variables:**\n",
    "  - Analizar y justificar la eliminación o transformación de columnas irrelevantes, constantes o ruidosas.\n",
    "  - Evaluar si es necesario aplicar técnicas de **normalización** o **estandarización** sobre las variables numéricas.\n",
    "    - Si se aplican, explicar qué método se usó (e.g., `StandardScaler`, `MinMaxScaler`) y por qué.\n",
    "    - Si no se aplican, justificar por qué no se consideran necesarias dadas las características del modelo y del dataset.\n",
    "\n",
    "- **Visualización (opcional pero recomendada):**\n",
    "  - Graficar las curvas de densidad estimadas para algunas variables por clase, con diferentes métodos de KDE.\n",
    "  - Mostrar gráficamente el efecto del parámetro `bandwidth` en la forma de la distribución.\n",
    "\n",
    "---\n",
    "\n",
    "## Entregables\n",
    "\n",
    "- **Código fuente documentado:** en formato Jupyter Notebook o scripts Python bien estructurados.\n",
    "- **Repositorio en GitHub:**  \n",
    "  - El proyecto completo debe entregarse en un repositorio público o privado (según indicaciones), incluyendo:\n",
    "    - Código fuente\n",
    "    - Archivo README con instrucciones de ejecución\n",
    "    - Presentación en formato `.pptx` o `.pdf`\n",
    "    - Cualquier recurso adicional necesario para reproducir los experimentos\n",
    "\n",
    "- **Presentación oral (15 + 5 minutos):**\n",
    "  - 15 minutos de exposición + 5 minutos de preguntas por parte del docente o compañeros.\n",
    "  - Debe incluir:\n",
    "    - Descripción clara de los métodos comparados\n",
    "    - Justificación del preprocesamiento realizado\n",
    "    - Visualizaciones de las distribuciones de verosimilitud (KDE) y curvas ROC\n",
    "    - Reporte de AUC promedio, luego de una validación cruzada de al menos 5 folds, de cada modelo analizado\n",
    "    - Análisis crítico: ¿usar KDE mejora la clasificación frente al modelo gaussiano tradicional? ¿bajo qué condiciones?\n",
    "    - Comparación de tiempos de computación entre los métodos.\n",
    "\n",
    "---\n",
    "\n",
    "## Recomendaciones\n",
    "\n",
    "- Leer artículos recientes que hayan trabajado con el **AI4I 2020 Predictive Maintenance Dataset**, en especial aquellos que discuten estrategias para manejar el **desbalance de clases**. Usar fuentes confiables como Google Scholar o Scopus.\n",
    "\n",
    "- Apoyarse en la **teoría revisada en clase** y en el **material del curso disponible en el repositorio oficial**. Revisar los notebooks de ejemplos, apuntes, y recursos adicionales proporcionados por el docente.\n",
    "\n",
    "- Se recomienda fuertemente el uso de herramientas de inteligencia artificial como **ChatGPT** o **GitHub Copilot** para:\n",
    "  - Aclarar dudas conceptuales.\n",
    "  - Obtener sugerencias de código.\n",
    "  - Redactar fragmentos documentados.\n",
    "  - Mejorar el entendimiento de errores o ajustes técnicos.\n",
    "\n",
    "> El uso de estas herramientas debe ser **ético, transparente y reflexivo**. No se trata de copiar código sin entenderlo, sino de apoyarse en IA como una forma de acelerar el trabajo, validar ideas y fomentar buenas prácticas.\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- Los artículos académicos utilizados como referencia deben ser mencionados al final de la presentación en formato **IEEE**.\n",
    "\n",
    "---\n",
    "\n",
    "## Fecha de entrega\n",
    "\n",
    "- **Límite de entrega:** domingo **22 de julio**, hasta las **23:59 (hora local)**.\n",
    "- El proyecto debe entregarse mediante un **repositorio en GitHub**, incluyendo:\n",
    "  - Código funcional y documentado\n",
    "  - Informe en formato `.pptx` o `.pdf`\n",
    "  - Instrucciones para reproducir los resultados\n",
    "\n",
    "---\n",
    "\n",
    "## Rúbrica de Evaluación (100 puntos)\n",
    "\n",
    "### Código y reporte técnico — 50 puntos\n",
    "\n",
    "| Criterio                                                                 | Puntos |\n",
    "|--------------------------------------------------------------------------|--------|\n",
    "| Implementación funcional de los tres métodos de KDE                      | 15     |\n",
    "| Optimización de `bandwidth` (KDE gaussiano) usando validación cruzada con AUC como métrica | 10     |\n",
    "| Comparación cuantitativa con GaussianNB usando AUC en validación cruzada (5 folds) | 5      |\n",
    "| Documentación clara en el notebook o scripts (comentarios, celdas explicativas, encabezados) | 10     |\n",
    "| Visualización y análisis de resultados (curvas de densidad, curvas ROC, resumen de AUC) | 5      |\n",
    "| Justificación y evidencia sobre el manejo del desbalance y preprocesamiento aplicado | 5      |\n",
    "\n",
    "\n",
    "### Presentación oral — 50 puntos\n",
    "\n",
    "| Criterio                                                                 | Puntos |\n",
    "|--------------------------------------------------------------------------|--------|\n",
    "| Explicación clara del enfoque Naive Bayes y motivación de usar KDE       | 10     |\n",
    "| Comparación detallada entre métodos de KDE y GaussianNB                  | 10     |\n",
    "| Justificación clara de decisiones de diseño y preprocesamiento           | 10     |\n",
    "| Análisis crítico de resultados                                          | 10     |\n",
    "| Claridad, manejo del tiempo, calidad visual de la presentación (diapositivas) | 10     |\n",
    "\n",
    "---\n",
    "\n",
    "**Total: 100 puntos**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
