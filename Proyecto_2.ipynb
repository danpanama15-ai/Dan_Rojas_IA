{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6974964c",
   "metadata": {},
   "source": [
    "# Clasificación de Piso en el Dataset UJIIndoorLoc\n",
    "\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"id\": \"#VSC-94d5bbd6\",\n",
    "      \"metadata\": {\n",
    "        \"language\": \"python\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Paso 5 (ejecutado aquí): Construir tabla resumen de hiperparámetros óptimos para cada modelo\\n\",\n",
    "        \"print('\\\\n=== Paso 5: Tabla de hiperparámetros óptimos ===')\\n\",\n",
    "        \"param_keys = ['n_neighbors','weights','var_smoothing','C','penalty','solver','max_iter','max_depth','min_samples_split','criterion','kernel','gamma','n_estimators','max_features']\\n\",\n",
    "        \"rows = []\\n\",\n",
    "        \"for name, est in best_models.items():\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        params = est.get_params()\\n\",\n",
    "        \"    except Exception:\\n\",\n",
    "        \"        params = {}\\n\",\n",
    "        \"    filtered = {k: params[k] for k in param_keys if k in params}\\n\",\n",
    "        \"    if filtered:\\n\",\n",
    "        \"        param_str = ', '.join(f\\\"{k}={filtered[k]}\\\" for k in filtered)\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        sample = dict(list(params.items())[:5])\\n\",\n",
    "        \"        param_str = ', '.join(f\\\"{k}={sample[k]}\\\" for k in sample) if sample else ''\\n\",\n",
    "        \"    rows.append({'Modelo': name, 'Hiperparámetros óptimos': param_str})\\n\",\n",
    "        \"df_hp = pd.DataFrame(rows)\\n\",\n",
    "        \"try:\\n\",\n",
    "        \"    print(df_hp.to_markdown(index=False))\\n\",\n",
    "        \"source\": [\n",
    "          \"# Paso 5 (ejecutado desde esta celda): Construir tabla resumen de hiperparámetros óptimos para cada modelo\\\\n\",\n",
    "          \"print('\\\\\\\\n=== Paso 5: Tabla de hiperparámetros óptimos ===')\\\\n\",\n",
    "          \"param_keys = ['n_neighbors','weights','var_smoothing','C','penalty','solver','max_iter','max_depth','min_samples_split','criterion','kernel','gamma','n_estimators','max_features']\\\\n\",\n",
    "          \"rows = []\\\\n\",\n",
    "          \"for name, est in best_models.items():\\\\n\",\n",
    "          \"    try:\\\\n\",\n",
    "          \"        params = est.get_params()\\\\n\",\n",
    "          \"    except Exception:\\\\n\",\n",
    "          \"        params = {}\\\\n\",\n",
    "          \"    filtered = {k: params[k] for k in param_keys if k in params}\\\\n\",\n",
    "          \"    if filtered:\\\\n\",\n",
    "          \"        param_str = ', '.join(f\\\\\"{k}={filtered[k]}\\\\\" for k in filtered)\\\\n\",\n",
    "          \"    else:\\\\n\",\n",
    "          \"        sample = dict(list(params.items())[:8])\\\\n\",\n",
    "          \"        param_str = ', '.join(f\\\\\"{k}={sample[k]}\\\\\" for k in sample) if sample else ''\\\\n\",\n",
    "          \"    rows.append({'Modelo': name, 'Hiperparámetros óptimos': param_str})\\\\n\",\n",
    "          \"df_hp = pd.DataFrame(rows)\\\\n\",\n",
    "          \"try:\\\\n\",\n",
    "          \"    print(df_hp.to_markdown(index=False))\\\\n\",\n",
    "          \"except Exception:\\\\n\",\n",
    "          \"    display(df_hp)\\\\n\",\n",
    "          \"display(df_hp)\\\\n\"\n",
    "        ]\n",
    "        \"# Lista de claves relevantes para mostrar (filtrado para legibilidad)\\n\",\n",
    "        \"param_keys = ['n_neighbors','weights','var_smoothing','C','penalty','solver','max_iter','max_depth','min_samples_split','criterion','kernel','gamma','n_estimators','max_features']\\n\",\n",
    "        \"rows = []\\n\",\n",
    "        \"for name, est in best_models.items():\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        params = est.get_params()\\n\",\n",
    "        \"    except Exception:\\n\",\n",
    "        \"        params = {}\\n\",\n",
    "        \"    filtered = {k: params[k] for k in param_keys if k in params}\\n\",\n",
    "        \"    if filtered:\\n\",\n",
    "        \"        param_str = ', '.join(f\\\"{k}={filtered[k]}\\\" for k in filtered)\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        # fallback: show first 5 params for traceability\\n\",\n",
    "        \"        sample = dict(list(params.items())[:5])\\n\",\n",
    "        \"        param_str = ', '.join(f\\\"{k}={sample[k]}\\\" for k in sample) if sample else ''\\n\",\n",
    "        \"    rows.append({'Modelo': name, 'Hiperparámetros óptimos': param_str})\\n\",\n",
    "        \"df_hp = pd.DataFrame(rows)\\n\",\n",
    "        \"# Mostrar como Markdown legible + DataFrame\\n\",\n",
    "        \"try:\\n\",\n",
    "        \"    print(df_hp.to_markdown(index=False))\\n\",\n",
    "        \"except Exception:\\n\",\n",
    "        \"    display(df_hp)\\n\",\n",
    "        \"display(df_hp)\\n\"\n",
    "      ]\n",
    "    },\n",
    "        \"        df_val = pd.read_csv(path)\\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        print(f\\\"No se pudo cargar {path}: {e}\\\")\\n\",\n",
    "        \"        return None, None\\n\",\n",
    "        \"    cols_to_drop = ['LONGITUDE', 'LATITUDE', 'SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP', 'BUILDINGID']\\n\",\n",
    "        \"    df_val = df_val.drop(columns=[c for c in cols_to_drop if c in df_val.columns], errors='ignore')\\n\",\n",
    "        \"    y_val = df_val['FLOOR']\\n\",\n",
    "        \"    X_val = df_val.drop('FLOOR', axis=1)\\n\",\n",
    "        \"    X_val[X_val == 100] = -100\\n\",\n",
    "        \"    if scaler_obj is not None:\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            X_val_scaled = scaler_obj.transform(X_val)\\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            print('Error al escalar validation set:', e)\\n\",\n",
    "        \"            X_val_scaled = X_val.values\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        X_val_scaled = X_val.values\\n\",\n",
    "        \"    return X_val_scaled, y_val\\n\"\n",
    "        \"print(\\\"\\\\n=== Entrenamiento GaussianNB ===\\\")\\n\",\n",
    "        \"from sklearn.naive_bayes import GaussianNB\\n\",\n",
    "        \"param_grid_gnb = { 'var_smoothing': np.logspace(-12, -6, 10) }\\n\",\n",
    "        \"grid_gnb = GridSearchCV(GaussianNB(), param_grid_gnb, cv=3, scoring='accuracy', n_jobs=-1)\\n\",\n",
    "        \"start = time.time()\\n\",\n",
    "        \"grid_gnb.fit(X_train_scaled, y_train)\\n\",\n",
    "        \"train_time = time.time() - start\\n\",\n",
    "        \"best_models['GaussianNB'] = grid_gnb.best_estimator_\\n\",\n",
    "        \"print(f\\\"Mejores parámetros GaussianNB: {grid_gnb.best_params_}\\\")\\n\",\n",
    "        \"print(f\\\"Best CV Accuracy GaussianNB: {grid_gnb.best_score_:.4f}\\\")\\n\",\n",
    "        \"print(f\\\"Tiempo de búsqueda GaussianNB: {train_time:.2f}s\\\")\\n\"\n",
    "## Objetivos\n",
    "\n",
    "- **Cargar y explorar** el conjunto de datos UJIIndoorLoc.\n",
    "- **Preparar** los datos seleccionando las características relevantes y el target (`FLOOR`).\n",
    "- **Dividir** el dataset en entrenamiento y validación (80/20).\n",
    "- **Entrenar y optimizar** clasificadores basados en seis algoritmos:\n",
    "  - K-Nearest Neighbors (KNN)\n",
    "  - Gaussian Naive Bayes\n",
    "  - Regresión Logística\n",
    "  - Árboles de Decisión\n",
    "  - Support Vector Machines (SVM)\n",
    "  - Random Forest\n",
    "- **Seleccionar hiperparámetros óptimos** para cada modelo utilizando validación cruzada (5-fold), empleando estrategias como **Grid Search**, **Randomized Search**, o **Bayesian Optimization** según el algoritmo.\n",
    "- **Comparar el desempeño** de los modelos sobre el conjunto de validación, usando métricas como *accuracy*, *precision*, *recall*, y *F1-score*.\n",
    "- **Determinar el mejor clasificador** para esta tarea, junto con sus hiperparámetros óptimos.\n",
    "\n",
    "Este ejercicio permite no solo evaluar la capacidad predictiva de distintos algoritmos clásicos de clasificación, sino también desarrollar buenas prácticas en validación de modelos y selección de hiperparámetros en contextos del mundo real.\n",
    "\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"id\": \"#VSC-94d5bbd6\",\n",
    "      \"metadata\": {\n",
    "        \"language\": \"python\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Entrenar y optimizar Gaussian Naive Bayes\\n\",\n",
    "        \"print(\\\"\\\\n=== Entrenamiento Gaussian Naive Bayes ===\\\")\\n\",\n",
    "        \"from sklearn.naive_bayes import GaussianNB\\n\",\n",
    "        \"param_grid_gnb = { 'var_smoothing': np.logspace(-12, -6, 12) }\\n\",\n",
    "        \"grid_gnb = GridSearchCV(GaussianNB(), param_grid_gnb, cv=3, scoring='accuracy', n_jobs=-1)\\n\",\n",
    "        \"start = time.time()\\n\",\n",
    "        \"# Preferir datos escalados si existen, sino usar datos crudos\\n\",\n",
    "        \"try:\\n\",\n",
    "        \"    grid_gnb.fit(X_train_scaled, y_train)\\n\",\n",
    "        \"except Exception:\\n\",\n",
    "        \"    grid_gnb.fit(X_train, y_train)\\n\",\n",
    "        \"train_time_gnb = time.time() - start\\n\",\n",
    "        \"best_models['GaussianNB'] = grid_gnb.best_estimator_\\n\",\n",
    "        \"print(f\\\"Mejores parámetros GaussianNB: {grid_gnb.best_params_}\\\")\\n\",\n",
    "        \"print(f\\\"Best CV Accuracy GaussianNB: {grid_gnb.best_score_:.4f}\\\")\\n\",\n",
    "        \"print(f\\\"Tiempo de búsqueda GaussianNB: {train_time_gnb:.2f}s\\\")\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"id\": \"#VSC-run-step5\",\n",
    "      \"metadata\": {\n",
    "        \"language\": \"python\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# (Celda añadida) Paso 5: Generar tabla resumen de hiperparámetros óptimos para cada modelo\\n\",\n",
    "        \"print('\\\\n=== Paso 5: Tabla de hiperparámetros óptimos (generada) ===')\\n\",\n",
    "        \"param_keys = ['n_neighbors','weights','var_smoothing','C','penalty','solver','max_iter','max_depth','min_samples_split','criterion','kernel','gamma','n_estimators','max_features']\\n\",\n",
    "        \"rows = []\\n\",\n",
    "        \"for name, est in best_models.items():\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        params = est.get_params()\\n\",\n",
    "        \"    except Exception:\\n\",\n",
    "        \"        params = {}\\n\",\n",
    "        \"    filtered = {k: params[k] for k in param_keys if k in params}\\n\",\n",
    "        \"    if filtered:\\n\",\n",
    "        \"        param_str = ', '.join(f\\\"{k}={filtered[k]}\\\" for k in filtered)\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        sample = dict(list(params.items())[:8])\\n\",\n",
    "        \"        param_str = ', '.join(f\\\"{k}={sample[k]}\\\" for k in sample) if sample else ''\\n\",\n",
    "        \"    rows.append({'Modelo': name, 'Hiperparámetros óptimos': param_str})\\n\",\n",
    "        \"df_hp = pd.DataFrame(rows)\\n\",\n",
    "        \"# Mostrar como Markdown + DataFrame\\n\",\n",
    "        \"try:\\n\",\n",
    "        \"    print(df_hp.to_markdown(index=False))\\n\",\n",
    "        \"except Exception:\\n\",\n",
    "        \"    display(df_hp)\\n\",\n",
    "        \"display(df_hp)\\n\"\n",
    "      ]\n",
    "    },\n",
    "        \"# helper: load validation set and preprocess consistently\\n\",\n",
    "        \"def load_and_preprocess_validation(path='validationData.csv', scaler_obj=None):\\n\",\n",
    "        \"    if not os.path.exists(path):\\n\",\n",
    "        \"        print(f'Archivo {path} no encontrado — se omitirá evaluación en validation set')\\n\",\n",
    "        \"        return None, None\\n\",\n",
    "        \"    df_val = pd.read_csv(path)\\n\",\n",
    "        \"    cols_to_drop = ['LONGITUDE', 'LATITUDE', 'SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP', 'BUILDINGID']\\n\",\n",
    "        \"    df_val = df_val.drop(columns=[c for c in cols_to_drop if c in df_val.columns], errors='ignore')\\n\",\n",
    "        \"    if 'FLOOR' not in df_val.columns:\\n\",\n",
    "        \"        print('validationData.csv no contiene columna FLOOR — omitiendo')\\n\",\n",
    "        \"        return None, None\\n\",\n",
    "        \"    y_val = df_val['FLOOR']\\n\",\n",
    "        \"    X_val = df_val.drop('FLOOR', axis=1)\\n\",\n",
    "        \"    X_val = X_val.copy()\\n\",\n",
    "        \"    X_val[X_val == 100] = -100\\n\",\n",
    "        \"    if scaler_obj is not None:\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            X_val_scaled = scaler_obj.transform(X_val)\\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            print('Error al escalar validation set:', e)\\n\",\n",
    "        \"            X_val_scaled = X_val.values\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        X_val_scaled = X_val.values\\n\",\n",
    "        \"    return X_val_scaled, y_val\\n\",\n",
    "        \"# cargar validation set (si existe)\\n\",\n",
    "        \"X_val_scaled, y_val = load_and_preprocess_validation('validationData.csv', scaler_obj=scaler if 'scaler' in globals() else None)\\n\",\n",
    "        \"results = []\\n\",\n",
    "        \"for name, est in best_models.items():\\n\",\n",
    "        \"    print(f'\\\\nEvaluando modelo: {name}')\\n\",\n",
    "        \"    # reentrenar en todo el conjunto de entrenamiento (usar X_train_scaled cuando sea apropiado)\\n\",\n",
    "        \"    start = time.time()\\n\",\n",
    "        \"    trained = False\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        est.fit(X_train_scaled, y_train)\\n\",\n",
    "        \"        trained = True\\n\",\n",
    "        \"    except Exception:\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            est.fit(X_train, y_train)\\n\",\n",
    "        \"            trained = True\\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            print('No se pudo reentrenar el modelo:', e)\\n\",\n",
    "        \"    train_time = time.time() - start\\n\",\n",
    "        \"    # predicción en test interno\\n\",\n",
    "        \"    start = time.time()\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        y_pred = est.predict(X_test_scaled)\\n\",\n",
    "        \"    except Exception:\\n\",\n",
    "        \"        y_pred = est.predict(X_test)\\n\",\n",
    "        \"    test_time = time.time() - start\\n\",\n",
    "        \"    acc = accuracy_score(y_test, y_pred)\\n\",\n",
    "        \"    prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\\n\",\n",
    "        \"    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\\n\",\n",
    "        \"    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\\n\",\n",
    "        \"    auc = None\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        if hasattr(est, 'predict_proba'):\\n\",\n",
    "        \"            y_score = est.predict_proba(X_test_scaled)\\n\",\n",
    "        \"            auc = roc_auc_score(y_test, y_score, multi_class='ovr')\\n\",\n",
    "        \"        elif hasattr(est, 'decision_function'):\\n\",\n",
    "        \"            y_score = est.decision_function(X_test_scaled)\\n\",\n",
    "        \"            auc = roc_auc_score(y_test, y_score, multi_class='ovr')\\n\",\n",
    "        \"    except Exception:\\n\",\n",
    "        \"        auc = None\\n\",\n",
    "        \"    results.append({'model': name, 'dataset': 'internal_test', 'accuracy': acc, 'precision_macro': prec, 'recall_macro': rec, 'f1_macro': f1, 'auc_ovr': auc, 'train_time_s': train_time, 'test_time_s': test_time})\\n\",\n",
    "        \"    # evaluación en validation set si está cargado\\n\",\n",
    "        \"    if X_val_scaled is not None:\\n\",\n",
    "        \"        start = time.time()\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            y_pred_val = est.predict(X_val_scaled)\\n\",\n",
    "        \"        except Exception:\\n\",\n",
    "        \"            y_pred_val = est.predict(X_val_scaled)\\n\",\n",
    "        \"        test_time_val = time.time() - start\\n\",\n",
    "        \"        acc_v = accuracy_score(y_val, y_pred_val)\\n\",\n",
    "        \"        prec_v = precision_score(y_val, y_pred_val, average='macro', zero_division=0)\\n\",\n",
    "        \"        rec_v = recall_score(y_val, y_pred_val, average='macro', zero_division=0)\\n\",\n",
    "        \"        f1_v = f1_score(y_val, y_pred_val, average='macro', zero_division=0)\\n\",\n",
    "        \"        auc_v = None\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            if hasattr(est, 'predict_proba'):\\n\",\n",
    "        \"                y_score_val = est.predict_proba(X_val_scaled)\\n\",\n",
    "        \"                auc_v = roc_auc_score(y_val, y_score_val, multi_class='ovr')\\n\",\n",
    "        \"        except Exception:\\n\",\n",
    "        \"            auc_v = None\\n\",\n",
    "        \"        results.append({'model': name, 'dataset': 'validation', 'accuracy': acc_v, 'precision_macro': prec_v, 'recall_macro': rec_v, 'f1_macro': f1_v, 'auc_ovr': auc_v, 'train_time_s': train_time, 'test_time_s': test_time_val})\\n\",\n",
    "        \"# Consolidar resultados y mostrarlos\\n\",\n",
    "        \"df_results = pd.DataFrame(results)\\n\",\n",
    "        \"print('\\\\n=== Resultados consolidados ===')\\n\",\n",
    "        \"display(df_results.sort_values(['model','dataset'], ascending=[True, True]))\\n\"\n",
    "      ]\n",
    "        \"from sklearn.naive_bayes import GaussianNB\\n\",\n",
    "        \"param_grid_gnb = { 'var_smoothing': np.logspace(-12, -6, 12) }\\n\",\n",
    "        \"grid_gnb = GridSearchCV(GaussianNB(), param_grid_gnb, cv=3, scoring='accuracy', n_jobs=-1)\\n\",\n",
    "        \"start = time.time()\\n\",\n",
    "        \"grid_gnb.fit(X_train_scaled, y_train)\\n\",\n",
    "        \"train_time_gnb = time.time() - start\\n\",\n",
    "        \"best_models['GaussianNB'] = grid_gnb.best_estimator_\\n\",\n",
    "        \"print(f\\\"Mejores parámetros GaussianNB: {grid_gnb.best_params_}\\\")\\n\",\n",
    "        \"print(f\\\"Best CV Accuracy GaussianNB: {grid_gnb.best_score_:.4f}\\\")\\n\",\n",
    "        \"print(f\\\"Tiempo búsqueda GaussianNB: {train_time_gnb:.2f}s\\\")\\n\"\n",
    "      ]\n",
    "        \"# Cargar validation set\\n\",\n",
    "        \"X_val_scaled, y_val = load_and_preprocess_validation('validationData.csv', scaler_obj=scaler)\\n\",\n",
    "        \"results = []\\n\",\n",
    "        \"for name, model in best_models.items():\\n\",\n",
    "        \"    print(f\\\"\\\\nEvaluando {name}...\\\")\\n\",\n",
    "        \"    est = model\\n\",\n",
    "        \"    # Reentrenar en todo X_train (usar escalado o no según modelo)\\n\",\n",
    "        \"    start = time.time()\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        est.fit(X_train_scaled if hasattr(est, 'fit') and X_train_scaled is not None else X_train, y_train)\\n\",\n",
    "        \"    except Exception:\\n\",\n",
    "        \"        # algunos árboles/ensemble pueden aceptar X_train directamente\\n\",\n",
    "        \"        est.fit(X_train, y_train)\\n\",\n",
    "        \"    train_time = time.time() - start\\n\",\n",
    "        \"    # Predicción interno\\n\",\n",
    "        \"    start = time.time()\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        y_pred = est.predict(X_test_scaled)\\n\",\n",
    "        \"    except Exception:\\n\",\n",
    "        \"        y_pred = est.predict(X_test)\\n\",\n",
    "        \"    test_time = time.time() - start\\n\",\n",
    "        \"    acc = accuracy_score(y_test, y_pred)\\n\",\n",
    "        \"    prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\\n\",\n",
    "        \"    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\\n\",\n",
    "        \"    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\\n\",\n",
    "        \"    # AUC (si es posible)\\n\",\n",
    "        \"    auc = None\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        if hasattr(est, 'predict_proba'):\\n\",\n",
    "        \"            y_score = est.predict_proba(X_test_scaled)\\n\",\n",
    "        \"            auc = roc_auc_score(y_test, y_score, multi_class='ovr')\\n\",\n",
    "        \"        elif hasattr(est, 'decision_function'):\\n\",\n",
    "        \"            y_score = est.decision_function(X_test_scaled)\\n\",\n",
    "        \"            auc = roc_auc_score(y_test, y_score, multi_class='ovr')\\n\",\n",
    "        \"    except Exception:\\n\",\n",
    "        \"        auc = None\\n\",\n",
    "        \"    results.append({'model': name, 'dataset': 'internal_test', 'accuracy': acc, 'precision_macro': prec, 'recall_macro': rec, 'f1_macro': f1, 'auc_ovr': auc, 'train_time_s': train_time, 'test_time_s': test_time})\\n\",\n",
    "        \"    # Evaluación en validation set si está disponible\\n\",\n",
    "        \"    if X_val_scaled is not None:\\n\",\n",
    "        \"        start = time.time()\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            y_pred_val = est.predict(X_val_scaled)\\n\",\n",
    "        \"        except Exception:\\n\",\n",
    "        \"            y_pred_val = est.predict(X_val_scaled)\\n\",\n",
    "        \"        test_time_val = time.time() - start\\n\",\n",
    "        \"        acc_v = accuracy_score(y_val, y_pred_val)\\n\",\n",
    "        \"        prec_v = precision_score(y_val, y_pred_val, average='macro', zero_division=0)\\n\",\n",
    "        \"        rec_v = recall_score(y_val, y_pred_val, average='macro', zero_division=0)\\n\",\n",
    "        \"        f1_v = f1_score(y_val, y_pred_val, average='macro', zero_division=0)\\n\",\n",
    "        \"        auc_v = None\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            if hasattr(est, 'predict_proba'):\\n\",\n",
    "        \"                y_score_val = est.predict_proba(X_val_scaled)\\n\",\n",
    "        \"                auc_v = roc_auc_score(y_val, y_score_val, multi_class='ovr')\\n\",\n",
    "        \"        except Exception:\\n\",\n",
    "        \"            auc_v = None\\n\",\n",
    "        \"        results.append({'model': name, 'dataset': 'validation', 'accuracy': acc_v, 'precision_macro': prec_v, 'recall_macro': rec_v, 'f1_macro': f1_v, 'auc_ovr': auc_v, 'train_time_s': train_time, 'test_time_s': test_time_val})\\n\",\n",
    "        \"# Consolidar resultados\\n\",\n",
    "        \"df_results = pd.DataFrame(results)\\n\",\n",
    "        \"print(\\\"\\\\n=== Resultados resumidos ===\\\")\\n\",\n",
    "        \"display(df_results.sort_values(['model','dataset']))\\n\"\n",
    "        \"print(\\\"\\\\n=== Entrenamiento Logistic Regression ===\\\")\\n\",\n",
    "        \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "        \"param_dist_lr = {\\n\",\n",
    "        \"    'C': np.logspace(-3, 3, 20),\\n\",\n",
    "        \"    'penalty': ['l2'],\\n\",\n",
    "        \"    'solver': ['lbfgs'],\\n\",\n",
    "        \"    'max_iter': [500, 1000]\\n\",\n",
    "        \"}\\n\",\n",
    "        \"rand_lr = RandomizedSearchCV(LogisticRegression(), param_dist_lr, n_iter=20, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\\n\",\n",
    "        \"start = time.time()\\n\",\n",
    "        \"rand_lr.fit(X_train_scaled, y_train)\\n\",\n",
    "        \"train_time = time.time() - start\\n\",\n",
    "        \"best_models['LogisticRegression'] = rand_lr.best_estimator_\\n\",\n",
    "        \"print(f\\\"Mejores parámetros LR: {rand_lr.best_params_}\\\")\\n\",\n",
    "        \"print(f\\\"Best CV Accuracy LR: {rand_lr.best_score_:.4f}\\\")\\n\",\n",
    "        \"print(f\\\"Tiempo búsqueda LR: {train_time:.2f}s\\\")\\n\"\n",
    "\n",
    "El objetivo en esta tarea es predecir el **piso** (`FLOOR`) en el que se encontraba el dispositivo en el momento de la medición, considerando únicamente las características numéricas provenientes de las señales WiFi.\n",
    "\n",
    "### Estructura del dataset\n",
    "\n",
    "- **Número de muestras**: ~20,000\n",
    "- **Número de características**: 520\n",
    "  - 520 columnas con valores de intensidad de señal WiFi (`WAP001` a `WAP520`)\n",
    "- **Variable objetivo**: `FLOOR` (variable categórica con múltiples clases, usualmente entre 0 y 4)\n",
    "\n",
    "### Columnas relevantes\n",
    "\n",
    "- `WAP001`, `WAP002`, ..., `WAP520`: niveles de señal recibida desde cada punto de acceso WiFi (valores entre -104 y 0, o 100 si no se detectó).\n",
    "- `FLOOR`: clase objetivo a predecir (nivel del edificio).\n",
    "- (Otras columnas como `BUILDINGID`, `SPACEID`, `USERID`, `TIMESTAMP`, etc., pueden ser ignoradas o utilizadas en análisis complementarios).\n",
    "\n",
    "### Contexto del problema\n",
    "\n",
    "La localización en interiores es un problema complejo en el que tecnologías como el GPS no funcionan adecuadamente. Los sistemas basados en WiFi han demostrado ser una alternativa efectiva para estimar la ubicación de usuarios en edificios. Poder predecir automáticamente el piso en el que se encuentra una persona puede mejorar aplicaciones de navegación en interiores, accesibilidad, gestión de emergencias y servicios personalizados. Este tipo de problemas es típicamente abordado mediante algoritmos de clasificación multiclase.\n",
    "\n",
    "\n",
    "### Estrategia de evaluación\n",
    "\n",
    "En este análisis seguiremos una metodología rigurosa para garantizar la validez de los resultados:\n",
    "\n",
    "1. **Dataset de entrenamiento**: Se utilizará exclusivamente para el desarrollo, entrenamiento y optimización de hiperparámetros de todos los modelos. Este conjunto será dividido internamente en subconjuntos de entrenamiento y validación (80/20) para la selección de hiperparámetros mediante validación cruzada.\n",
    "\n",
    "2. **Dataset de prueba**: Se reservará únicamente para la **evaluación final** de los modelos ya optimizados. Este conjunto **no debe ser utilizado** durante el proceso de selección de hiperparámetros, ajuste de modelos o toma de decisiones sobre la arquitectura, ya que esto introduciría sesgo y comprometería la capacidad de generalización estimada.\n",
    "\n",
    "3. **Validación cruzada**: Para la optimización de hiperparámetros se empleará validación cruzada 5-fold sobre el conjunto de entrenamiento, lo que permitirá una estimación robusta del rendimiento sin contaminar los datos de prueba.\n",
    "\n",
    "Esta separación estricta entre datos de desarrollo y evaluación final es fundamental para obtener una estimación realista del rendimiento que los modelos tendrían en un escenario de producción con datos completamente nuevos.\n",
    "\n",
    "        {\n",
    "          \"cell_type\": \"code\",\n",
    "          \"id\": \"#VSC-ebaaa726\",\n",
    "          \"metadata\": {\n",
    "            \"language\": \"python\"\n",
    "          },\n",
    "          \"source\": [\n",
    "            \"# Paso 7: Evaluar modelos optimizados en el conjunto de prueba\\n\",\n",
    "            \"print(\\\\\"\\\\n=== PASO 7: Evaluaci\\u00f3n de modelos optimizados ===\\\\\")\\n\",\n",
    "            \"import os, pickle\\n\",\n",
    "            \"# Asegurar que GaussianNB cargado desde archivo si existe\\n\",\n",
    "            \"if 'GaussianNB' not in best_models:\\n\",\n",
    "            \"    if os.path.exists('best_gaussiannb.pkl'):\\n\",\n",
    "            \"        with open('best_gaussiannb.pkl', 'rb') as f:\\n\",\n",
    "            \"            best_models['GaussianNB'] = pickle.load(f)\\n\",\n",
    "            \"        print('Cargado best_gaussiannb.pkl en best_models[\\\\'GaussianNB\\\\']')\\n\",\n",
    "            \"    else:\\n\",\n",
    "            \"        print('best_gaussiannb.pkl no encontrado \t6 GaussianNB omitido si no est\\u00e1 en best_models')\\n\",\n",
    "            \"\\n\",\n",
    "            \"# Helper para cargar validation set si existe\\n\",\n",
    "            \"def load_validation(path='validationData.csv', scaler_obj=None):\\n\",\n",
    "            \"    if not os.path.exists(path):\\n\",\n",
    "            \"        return None, None\\n\",\n",
    "            \"    df_val = pd.read_csv(path)\\n\",\n",
    "            \"    cols_to_drop = ['LONGITUDE','LATITUDE','SPACEID','RELATIVEPOSITION','USERID','PHONEID','TIMESTAMP','BUILDINGID']\\n\",\n",
    "            \"    df_val = df_val.drop(columns=[c for c in cols_to_drop if c in df_val.columns], errors='ignore')\\n\",\n",
    "            \"    if 'FLOOR' not in df_val.columns:\\n\",\n",
    "            \"        return None, None\\n\",\n",
    "            \"    y_val = df_val['FLOOR']\\n\",\n",
    "            \"    X_val = df_val.drop('FLOOR', axis=1).copy()\\n\",\n",
    "            \"    X_val[X_val == 100] = -100\\n\",\n",
    "            \"    if scaler_obj is not None:\\n\",\n",
    "            \"        try:\\n\",\n",
    "            \"            X_val_scaled = scaler_obj.transform(X_val)\\n\",\n",
    "            \"        except Exception:\\n\",\n",
    "            \"            X_val_scaled = X_val.values\\n\",\n",
    "            \"    else:\\n\",\n",
    "            \"        X_val_scaled = X_val.values\\n\",\n",
    "            \"    return X_val_scaled, y_val\\n\",\n",
    "            \"\\n\",\n",
    "            \"# Cargar validation set si existe\\n\",\n",
    "            \"X_val_scaled, y_val = load_validation('validationData.csv', scaler_obj=scaler if 'scaler' in globals() else None)\\n\",\n",
    "            \"\\n\",\n",
    "            \"results = []\\n\",\n",
    "            \"for name, est in best_models.items():\\n\",\n",
    "            \"    print(f'\\\\nEvaluando modelo: {name}')\\n\",\n",
    "            \"    # Reentrenar en todo el conjunto de entrenamiento (usar X_train_scaled cuando sea apropiado)\\n\",\n",
    "            \"    start = time.time()\\n\",\n",
    "            \"    try:\\n\",\n",
    "            \"        est.fit(X_train_scaled, y_train)\\n\",\n",
    "            \"    except Exception:\\n\",\n",
    "            \"        try:\\n\",\n",
    "            \"            est.fit(X_train, y_train)\\n\",\n",
    "            \"        except Exception as e:\\n\",\n",
    "            \"            print('No se pudo reentrenar el modelo:', e)\\n\",\n",
    "            \"    train_time = time.time() - start\\n\",\n",
    "            \"    # Predicci\\u00f3n en test interno\\n\",\n",
    "            \"    start = time.time()\\n\",\n",
    "            \"    try:\\n\",\n",
    "            \"        y_pred = est.predict(X_test_scaled)\\n\",\n",
    "            \"    except Exception:\\n\",\n",
    "            \"        y_pred = est.predict(X_test)\\n\",\n",
    "            \"    test_time = time.time() - start\\n\",\n",
    "            \"    acc = accuracy_score(y_test, y_pred)\\n\",\n",
    "            \"    prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\\n\",\n",
    "            \"    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\\n\",\n",
    "            \"    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\\n\",\n",
    "            \"    auc = None\\n\",\n",
    "            \"    try:\\n\",\n",
    "            \"        if hasattr(est, 'predict_proba'):\\n\",\n",
    "            \"            y_score = est.predict_proba(X_test_scaled)\\n\",\n",
    "            \"            auc = roc_auc_score(y_test, y_score, multi_class='ovr')\\n\",\n",
    "            \"        elif hasattr(est, 'decision_function'):\\n\",\n",
    "            \"            y_score = est.decision_function(X_test_scaled)\\n\",\n",
    "            \"            auc = roc_auc_score(y_test, y_score, multi_class='ovr')\\n\",\n",
    "            \"    except Exception:\\n\",\n",
    "            \"        auc = None\\n\",\n",
    "            \"    results.append({'model': name, 'dataset': 'internal_test', 'accuracy': acc, 'precision_macro': prec, 'recall_macro': rec, 'f1_macro': f1, 'auc_ovr': auc, 'train_time_s': train_time, 'test_time_s': test_time})\\n\",\n",
    "            \"    # Evaluaci\\u00f3n en validation set si est\\u00e1 disponible\\n\",\n",
    "            \"    if X_val_scaled is not None:\\n\",\n",
    "            \"        start = time.time()\\n\",\n",
    "            \"        try:\\n\",\n",
    "            \"            y_pred_val = est.predict(X_val_scaled)\\n\",\n",
    "            \"        except Exception:\\n\",\n",
    "            \"            y_pred_val = est.predict(X_val_scaled)\\n\",\n",
    "            \"        test_time_val = time.time() - start\\n\",\n",
    "            \"        acc_v = accuracy_score(y_val, y_pred_val)\\n\",\n",
    "            \"        prec_v = precision_score(y_val, y_pred_val, average='macro', zero_division=0)\\n\",\n",
    "            \"        rec_v = recall_score(y_val, y_pred_val, average='macro', zero_division=0)\\n\",\n",
    "            \"        f1_v = f1_score(y_val, y_pred_val, average='macro', zero_division=0)\\n\",\n",
    "            \"        auc_v = None\\n\",\n",
    "            \"        try:\\n\",\n",
    "            \"            if hasattr(est, 'predict_proba'):\\n\",\n",
    "            \"                y_score_val = est.predict_proba(X_val_scaled)\\n\",\n",
    "            \"                auc_v = roc_auc_score(y_val, y_score_val, multi_class='ovr')\\n\",\n",
    "            \"        except Exception:\\n\",\n",
    "            \"            auc_v = None\\n\",\n",
    "            \"        results.append({'model': name, 'dataset': 'validation', 'accuracy': acc_v, 'precision_macro': prec_v, 'recall_macro': rec_v, 'f1_macro': f1_v, 'auc_ovr': auc_v, 'train_time_s': train_time, 'test_time_s': test_time_val})\\n\",\n",
    "            \"# Consolidar resultados y mostrarlos\\n\",\n",
    "            \"df_results = pd.DataFrame(results)\\n\",\n",
    "            \"print(\\\\\"\\\\n=== Resultados consolidados ===\\\\\")\\n\",\n",
    "            \"display(df_results.sort_values(['model','dataset'], ascending=[True, True]))\\n\"\n",
    "          ]\n",
    "        },\n",
    "        \"param_dist_dt = {\\n\",\n",
    "        \"    'max_depth': [None, 5, 10, 20, 30],\\n\",\n",
    "        \"    'min_samples_split': [2, 5, 10],\\n\",\n",
    "        \"    'criterion': ['gini', 'entropy']\\n\",\n",
    "        \"}\\n\",\n",
    "        \"rand_dt = RandomizedSearchCV(DecisionTreeClassifier(random_state=42), param_dist_dt, n_iter=20, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\\n\",\n",
    "        \"start = time.time()\\n\",\n",
    "        \"rand_dt.fit(X_train, y_train)\\n\",\n",
    "        \"train_time = time.time() - start\\n\",\n",
    "        \"best_models['DecisionTree'] = rand_dt.best_estimator_\\n\",\n",
    "        \"print(f\\\"Mejores parámetros DecisionTree: {rand_dt.best_params_}\\\")\\n\",\n",
    "        \"print(f\\\"Best CV Accuracy DecisionTree: {rand_dt.best_score_:.4f}\\\")\\n\",\n",
    "        \"print(f\\\"Tiempo búsqueda DecisionTree: {train_time:.2f}s\\\")\\n\"\n",
    "- Carga el dataset utilizando `pandas`.\n",
    "- Muestra las primeras filas del dataset utilizando `df.head()`.\n",
    "- Imprime el número total de muestras (filas) y características (columnas).\n",
    "- Verifica cuántas clases distintas hay en la variable objetivo `FLOOR` y cuántas muestras tiene cada clase (`df['FLOOR'].value_counts()`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d27f8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASO 1: Cargar y explorar el dataset ===\n",
      "\n",
      "Forma del dataset: (19937, 529)\n",
      "\n",
      "Primeras 5 filas:\n",
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0     100     100     100     100     100     100     100     100     100   \n",
      "1     100     100     100     100     100     100     100     100     100   \n",
      "2     100     100     100     100     100     100     100     -97     100   \n",
      "3     100     100     100     100     100     100     100     100     100   \n",
      "4     100     100     100     100     100     100     100     100     100   \n",
      "\n",
      "   WAP010  ...  WAP520  LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0     100  ...     100 -7541.2643  4.864921e+06      2           1      106   \n",
      "1     100  ...     100 -7536.6212  4.864934e+06      2           1      106   \n",
      "2     100  ...     100 -7519.1524  4.864950e+06      2           1      103   \n",
      "3     100  ...     100 -7524.5704  4.864934e+06      2           1      102   \n",
      "4     100  ...     100 -7632.1436  4.864982e+06      0           0      122   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID   TIMESTAMP  \n",
      "0                 2       2       23  1371713733  \n",
      "1                 2       2       23  1371713691  \n",
      "2                 2       2       23  1371714095  \n",
      "3                 2       2       23  1371713807  \n",
      "4                 2      11       13  1369909710  \n",
      "\n",
      "[5 rows x 529 columns]\n",
      "\n",
      "Nombre de columnas (primeras 10): ['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005', 'WAP006', 'WAP007', 'WAP008', 'WAP009', 'WAP010']\n",
      "\n",
      "Tipo de datos:\n",
      "int64      527\n",
      "float64      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución de FLOOR:\n",
      "FLOOR\n",
      "0    4369\n",
      "1    5002\n",
      "2    4416\n",
      "3    5048\n",
      "4    1102\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total de clases en FLOOR: 5\n",
      "Forma del dataset: (19937, 529)\n",
      "\n",
      "Primeras 5 filas:\n",
      "   WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  WAP008  WAP009  \\\n",
      "0     100     100     100     100     100     100     100     100     100   \n",
      "1     100     100     100     100     100     100     100     100     100   \n",
      "2     100     100     100     100     100     100     100     -97     100   \n",
      "3     100     100     100     100     100     100     100     100     100   \n",
      "4     100     100     100     100     100     100     100     100     100   \n",
      "\n",
      "   WAP010  ...  WAP520  LONGITUDE      LATITUDE  FLOOR  BUILDINGID  SPACEID  \\\n",
      "0     100  ...     100 -7541.2643  4.864921e+06      2           1      106   \n",
      "1     100  ...     100 -7536.6212  4.864934e+06      2           1      106   \n",
      "2     100  ...     100 -7519.1524  4.864950e+06      2           1      103   \n",
      "3     100  ...     100 -7524.5704  4.864934e+06      2           1      102   \n",
      "4     100  ...     100 -7632.1436  4.864982e+06      0           0      122   \n",
      "\n",
      "   RELATIVEPOSITION  USERID  PHONEID   TIMESTAMP  \n",
      "0                 2       2       23  1371713733  \n",
      "1                 2       2       23  1371713691  \n",
      "2                 2       2       23  1371714095  \n",
      "3                 2       2       23  1371713807  \n",
      "4                 2      11       13  1369909710  \n",
      "\n",
      "[5 rows x 529 columns]\n",
      "\n",
      "Nombre de columnas (primeras 10): ['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005', 'WAP006', 'WAP007', 'WAP008', 'WAP009', 'WAP010']\n",
      "\n",
      "Tipo de datos:\n",
      "int64      527\n",
      "float64      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribución de FLOOR:\n",
      "FLOOR\n",
      "0    4369\n",
      "1    5002\n",
      "2    4416\n",
      "3    5048\n",
      "4    1102\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total de clases en FLOOR: 5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paso 1: Cargar y explorar el dataset\n",
    "print(\"=== PASO 1: Cargar y explorar el dataset ===\\n\")\n",
    "df = pd.read_csv('trainingData.csv')\n",
    "\n",
    "print(f\"Forma del dataset: {df.shape}\")\n",
    "print(f\"\\nPrimeras 5 filas:\\n{df.head()}\")\n",
    "print(f\"\\nNombre de columnas (primeras 10): {df.columns[:10].tolist()}\")\n",
    "print(f\"\\nTipo de datos:\\n{df.dtypes.value_counts()}\")\n",
    "\n",
    "# Explorar variable objetivo FLOOR\n",
    "print(f\"\\nDistribución de FLOOR:\")\n",
    "print(df['FLOOR'].value_counts().sort_index())\n",
    "print(f\"\\nTotal de clases en FLOOR: {df['FLOOR'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f0bed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 2: Preparar los datos\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Elimina las columnas que no son relevantes para la tarea de clasificación del piso:\n",
    "        \"# Entrenar SVM (RandomizedSearch) — puede ser lento, uso n_iter reducido\\n\",\n",
    "        \"print(\\\"\\\\n=== Entrenamiento SVM ===\\\")\\n\",\n",
    "        \"from sklearn.svm import SVC\\n\",\n",
    "        \"param_dist_svm = {\\n\",\n",
    "        \"    'C': np.logspace(-2, 2, 10),\\n\",\n",
    "        \"    'kernel': ['rbf', 'linear'],\\n\",\n",
    "        \"    'gamma': ['scale', 'auto']\\n\",\n",
    "        \"}\\n\",\n",
    "        \"rand_svm = RandomizedSearchCV(SVC(probability=True), param_dist_svm, n_iter=12, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\\n\",\n",
    "        \"start = time.time()\\n\",\n",
    "        \"rand_svm.fit(X_train_scaled, y_train)\\n\",\n",
    "        \"train_time = time.time() - start\\n\",\n",
    "        \"best_models['SVM'] = rand_svm.best_estimator_\\n\",\n",
    "        \"print(f\\\"Mejores parámetros SVM: {rand_svm.best_params_}\\\")\\n\",\n",
    "        \"print(f\\\"Best CV Accuracy SVM: {rand_svm.best_score_:.4f}\\\")\\n\",\n",
    "        \"print(f\\\"Tiempo búsqueda SVM: {train_time:.2f}s\\\")\\n\"\n",
    "- Conserva únicamente:\n",
    "  - Las columnas `WAP001` a `WAP520` como características (RSSI de puntos de acceso WiFi).\n",
    "  - La columna `FLOOR` como variable objetivo.\n",
    "- Verifica si existen valores atípicos o valores inválidos en las señales WiFi (por ejemplo: valores constantes como 100 o -110 que suelen indicar ausencia de señal).\n",
    "- Separa el conjunto de datos en:\n",
    "  - `X`: matriz de características (todas las columnas `WAP`)\n",
    "  - `y`: vector objetivo (`FLOOR`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f0eec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PASO 2: Preparar los datos ===\n",
      "\n",
      "Forma después de eliminar columnas: (19937, 521)\n",
      "Columnas restantes (muestra): ['WAP001', 'WAP002', 'WAP003', 'WAP004', 'WAP005', 'WAP006', 'WAP007', 'WAP008', 'WAP009', 'WAP010']\n",
      "\n",
      "Forma de X: (19937, 520)\n",
      "Forma de y: (19937,)\n",
      "Valores únicos en y: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "\n",
      "Valores de X - estadísticas:\n",
      "             WAP001        WAP002   WAP003   WAP004        WAP005  \\\n",
      "count  19937.000000  19937.000000  19937.0  19937.0  19937.000000   \n",
      "mean      99.823644     99.820936    100.0    100.0     99.613733   \n",
      "std        5.866842      5.798156      0.0      0.0      8.615657   \n",
      "min      -97.000000    -90.000000    100.0    100.0    -97.000000   \n",
      "25%      100.000000    100.000000    100.0    100.0    100.000000   \n",
      "50%      100.000000    100.000000    100.0    100.0    100.000000   \n",
      "75%      100.000000    100.000000    100.0    100.0    100.000000   \n",
      "max      100.000000    100.000000    100.0    100.0    100.000000   \n",
      "\n",
      "             WAP006        WAP007        WAP008        WAP009        WAP010  \\\n",
      "count  19937.000000  19937.000000  19937.000000  19937.000000  19937.000000   \n",
      "mean      97.130461     94.733661     93.820234     94.693936     99.163766   \n",
      "std       22.931890     30.541335     33.010404     30.305084     12.634045   \n",
      "min      -98.000000    -99.000000    -98.000000    -98.000000    -99.000000   \n",
      "25%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "50%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "75%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "max      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "\n",
      "       ...        WAP511        WAP512        WAP513        WAP514  \\\n",
      "count  ...  19937.000000  19937.000000  19937.000000  19937.000000   \n",
      "mean   ...     84.974921     99.866881     98.881276     98.833325   \n",
      "std    ...     50.099899      5.023663     14.206023     14.634613   \n",
      "min    ...   -103.000000    -97.000000    -92.000000    -90.000000   \n",
      "25%    ...    100.000000    100.000000    100.000000    100.000000   \n",
      "50%    ...    100.000000    100.000000    100.000000    100.000000   \n",
      "75%    ...    100.000000    100.000000    100.000000    100.000000   \n",
      "max    ...    100.000000    100.000000    100.000000    100.000000   \n",
      "\n",
      "             WAP515        WAP516        WAP517        WAP518        WAP519  \\\n",
      "count  19937.000000  19937.000000  19937.000000  19937.000000  19937.000000   \n",
      "mean      99.436525     73.705673     59.889803     99.788634     99.970507   \n",
      "std       10.259137     62.278292     74.060259      6.360671      2.404232   \n",
      "min      -97.000000   -101.000000   -101.000000    -97.000000    -97.000000   \n",
      "25%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "50%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "75%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "max      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "\n",
      "        WAP520  \n",
      "count  19937.0  \n",
      "mean     100.0  \n",
      "std        0.0  \n",
      "min      100.0  \n",
      "25%      100.0  \n",
      "50%      100.0  \n",
      "75%      100.0  \n",
      "max      100.0  \n",
      "\n",
      "[8 rows x 520 columns]\n",
      "             WAP001        WAP002   WAP003   WAP004        WAP005  \\\n",
      "count  19937.000000  19937.000000  19937.0  19937.0  19937.000000   \n",
      "mean      99.823644     99.820936    100.0    100.0     99.613733   \n",
      "std        5.866842      5.798156      0.0      0.0      8.615657   \n",
      "min      -97.000000    -90.000000    100.0    100.0    -97.000000   \n",
      "25%      100.000000    100.000000    100.0    100.0    100.000000   \n",
      "50%      100.000000    100.000000    100.0    100.0    100.000000   \n",
      "75%      100.000000    100.000000    100.0    100.0    100.000000   \n",
      "max      100.000000    100.000000    100.0    100.0    100.000000   \n",
      "\n",
      "             WAP006        WAP007        WAP008        WAP009        WAP010  \\\n",
      "count  19937.000000  19937.000000  19937.000000  19937.000000  19937.000000   \n",
      "mean      97.130461     94.733661     93.820234     94.693936     99.163766   \n",
      "std       22.931890     30.541335     33.010404     30.305084     12.634045   \n",
      "min      -98.000000    -99.000000    -98.000000    -98.000000    -99.000000   \n",
      "25%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "50%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "75%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "max      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "\n",
      "       ...        WAP511        WAP512        WAP513        WAP514  \\\n",
      "count  ...  19937.000000  19937.000000  19937.000000  19937.000000   \n",
      "mean   ...     84.974921     99.866881     98.881276     98.833325   \n",
      "std    ...     50.099899      5.023663     14.206023     14.634613   \n",
      "min    ...   -103.000000    -97.000000    -92.000000    -90.000000   \n",
      "25%    ...    100.000000    100.000000    100.000000    100.000000   \n",
      "50%    ...    100.000000    100.000000    100.000000    100.000000   \n",
      "75%    ...    100.000000    100.000000    100.000000    100.000000   \n",
      "max    ...    100.000000    100.000000    100.000000    100.000000   \n",
      "\n",
      "             WAP515        WAP516        WAP517        WAP518        WAP519  \\\n",
      "count  19937.000000  19937.000000  19937.000000  19937.000000  19937.000000   \n",
      "mean      99.436525     73.705673     59.889803     99.788634     99.970507   \n",
      "std       10.259137     62.278292     74.060259      6.360671      2.404232   \n",
      "min      -97.000000   -101.000000   -101.000000    -97.000000    -97.000000   \n",
      "25%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "50%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "75%      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "max      100.000000    100.000000    100.000000    100.000000    100.000000   \n",
      "\n",
      "        WAP520  \n",
      "count  19937.0  \n",
      "mean     100.0  \n",
      "std        0.0  \n",
      "min      100.0  \n",
      "25%      100.0  \n",
      "50%      100.0  \n",
      "75%      100.0  \n",
      "max      100.0  \n",
      "\n",
      "[8 rows x 520 columns]\n"
     ]
    }
   ],
   "source": [
    "# Paso 2: Preparar los datos\n",
    "print(\"\\n=== PASO 2: Preparar los datos ===\\n\")\n",
    "\n",
    "# Columnas a eliminar\n",
    "cols_to_drop = ['LONGITUDE', 'LATITUDE', 'SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP', 'BUILDINGID']\n",
    "df_clean = df.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Forma después de eliminar columnas: {df_clean.shape}\")\n",
    "print(f\"Columnas restantes (muestra): {df_clean.columns[:10].tolist()}\")\n",
    "\n",
    "# Separar X (características) y y (objetivo)\n",
    "y = df_clean['FLOOR']\n",
    "X = df_clean.drop('FLOOR', axis=1)\n",
    "\n",
    "print(f\"\\nForma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "print(f\"Valores únicos en y: {sorted(y.unique())}\")\n",
    "print(f\"\\nValores de X - estadísticas:\")\n",
    "print(X.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a6c39",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Paso 3: Preprocesamiento de las señales WiFi\n",
    "\n",
    "**Contexto:**\n",
    "\n",
    "Las columnas `WAP001` a `WAP520` representan la intensidad de la señal (RSSI) recibida desde distintos puntos de acceso WiFi. Los valores típicos de RSSI están en una escala negativa, donde:\n",
    "        \"# Entrenar Random Forest (RandomizedSearch)\\n\",\n",
    "        \"print(\\\"\\\\n=== Entrenamiento Random Forest ===\\\")\\n\",\n",
    "        \"from sklearn.ensemble import RandomForestClassifier\\n\",\n",
    "        \"param_dist_rf = {\\n\",\n",
    "        \"    'n_estimators': [50, 100, 200],\\n\",\n",
    "        \"    'max_depth': [None, 10, 20, 30],\\n\",\n",
    "        \"    'max_features': ['sqrt', 'log2']\\n\",\n",
    "        \"}\\n\",\n",
    "        \"rand_rf = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_dist_rf, n_iter=20, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\\n\",\n",
    "        \"start = time.time()\\n\",\n",
    "        \"rand_rf.fit(X_train, y_train)\\n\",\n",
    "        \"train_time = time.time() - start\\n\",\n",
    "        \"best_models['RandomForest'] = rand_rf.best_estimator_\\n\",\n",
    "        \"print(f\\\"Mejores parámetros RF: {rand_rf.best_params_}\\\")\\n\",\n",
    "        \"print(f\\\"Best CV Accuracy RF: {rand_rf.best_score_:.4f}\\\")\\n\",\n",
    "        \"print(f\\\"Tiempo búsqueda RF: {train_time:.2f}s\\\")\\n\"\n",
    "- Valores cercanos a **0 dBm** indican señal fuerte.\n",
    "- Valores cercanos a **-100 dBm** indican señal débil o casi ausente.\n",
    "- Un valor de **100** en este dataset representa una señal **no detectada**, es decir, el punto de acceso no fue visto por el dispositivo en ese instante.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Para facilitar el procesamiento y tratar la ausencia de señal de forma coherente, se recomienda mapear todos los valores **100** a **-100**, que semánticamente representa *ausencia de señal detectable*.\n",
    "- Esto unifica el rango de valores y evita que 100 (un valor artificial) afecte negativamente la escala de los algoritmos.\n",
    "\n",
    "**Pasos sugeridos:**\n",
    "\n",
    "- Reemplaza todos los valores `100` por `-100` en las columnas `WAP001` a `WAP520`:\n",
    "  ```python\n",
    "  X[X == 100] = -100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61fa6fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PASO 3: Preprocesamiento de señales WiFi ===\n",
      "\n",
      "Cantidad de valores 100 en X: 10008477\n",
      "Cantidad de valores 100 después del reemplazo: 0\n",
      "Cantidad de valores -100 después del reemplazo: 10008716\n",
      "\n",
      "Rango de valores en X_processed: -104 a 0\n",
      "Cantidad de valores 100 después del reemplazo: 0\n",
      "Cantidad de valores -100 después del reemplazo: 10008716\n",
      "\n",
      "Rango de valores en X_processed: -104 a 0\n"
     ]
    }
   ],
   "source": [
    "# Paso 3: Preprocesamiento - reemplazar valores 100 por -100\n",
    "print(\"\\n=== PASO 3: Preprocesamiento de señales WiFi ===\\n\")\n",
    "\n",
    "# Valores antes del reemplazo\n",
    "print(f\"Cantidad de valores 100 en X: {(X == 100).sum().sum()}\")\n",
    "\n",
    "# Reemplazar 100 por -100 (ausencia de señal)\n",
    "X_processed = X.copy()\n",
    "X_processed[X_processed == 100] = -100\n",
    "\n",
    "print(f\"Cantidad de valores 100 después del reemplazo: {(X_processed == 100).sum().sum()}\")\n",
    "print(f\"Cantidad de valores -100 después del reemplazo: {(X_processed == -100).sum().sum()}\")\n",
    "print(f\"\\nRango de valores en X_processed: {X_processed.min().min()} a {X_processed.max().max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80383336",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Paso 4: Entrenamiento y optimización de hiperparámetros\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Entrenar y comparar distintos clasificadores para predecir correctamente el piso (`FLOOR`) y encontrar los mejores hiperparámetros para cada uno mediante validación cruzada.\n",
    "\n",
    "**Clasificadores a evaluar:**\n",
    "\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Gaussian Naive Bayes\n",
    "- Regresión Logística\n",
    "- Árboles de Decisión\n",
    "- Support Vector Machines (SVM)\n",
    "- Random Forest\n",
    "\n",
    "**Procedimiento:**\n",
    "\n",
    "1. Divide el dataset en conjunto de **entrenamiento** (80%) y **validación** (20%) usando `train_test_split` con `stratify=y`.\n",
    "2. Para cada clasificador:\n",
    "   - Define el espacio de búsqueda de hiperparámetros.\n",
    "   - Usa **validación cruzada 5-fold** sobre el conjunto de entrenamiento para seleccionar los mejores hiperparámetros.\n",
    "   - Emplea una estrategia de búsqueda adecuada:\n",
    "     - **GridSearchCV**: búsqueda exhaustiva (ideal para espacios pequeños).\n",
    "     - **RandomizedSearchCV**: búsqueda aleatoria (más eficiente con espacios amplios).\n",
    "     - **Bayesian Optimization** (opcional): para búsquedas más inteligentes, usando librerías como `optuna` o `skopt`.\n",
    "3. Guarda el mejor modelo encontrado para cada clasificador con su configuración óptima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f87cd1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PASO 4: División de datos y entrenamiento ===\n",
      "\n",
      "X_train shape: (15949, 520)\n",
      "X_test shape: (3988, 520)\n",
      "y_train shape: (15949,)\n",
      "y_test shape: (3988,)\n",
      "\n",
      "Datos escalados - X_train_scaled shape: (15949, 520)\n",
      "\n",
      "Datos escalados - X_train_scaled shape: (15949, 520)\n"
     ]
    }
   ],
   "source": [
    "# Paso 4: División de datos en entrenamiento y validación (80/20)\n",
    "print(\"\\n=== PASO 4: División de datos y entrenamiento ===\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nDatos escalados - X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "\n",
    "# Diccionario para almacenar los mejores modelos\n",
    "best_models = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "850e98ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Entrenamiento KNN ===\n",
      "Mejores parámetros KNN: {'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best CV Accuracy KNN: 0.9878\n",
      "Tiempo de entrenamiento: 29.75s\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento de KNN con GridSearchCV\n",
    "print(\"\\n=== Entrenamiento KNN ===\")\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid_knn,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_knn.fit(X_train_scaled, y_train)\n",
    "train_time_knn = time.time() - start_time\n",
    "\n",
    "best_models['KNN'] = grid_knn.best_estimator_\n",
    "print(f\"Mejores parámetros KNN: {grid_knn.best_params_}\")\n",
    "print(f\"Best CV Accuracy KNN: {grid_knn.best_score_:.4f}\")\n",
    "print(f\"Tiempo de entrenamiento: {train_time_knn:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc6ca0",
   "metadata": {},
   "source": [
    "# train and optimize Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da7bc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and optimize Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fca347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and optimize decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cdbe3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and optimize Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38458a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and optimize Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de7a7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 5: Crear una tabla resumen de los mejores modelos\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "Después de entrenar y optimizar todos los clasificadores, debes construir una **tabla resumen en formato Markdown** que incluya:\n",
    "\n",
    "- El **nombre del modelo**\n",
    "- Los **hiperparámetros óptimos** encontrados mediante validación cruzada\n",
    "\n",
    "### Requisitos:\n",
    "\n",
    "- La tabla debe estar escrita en formato **Markdown**.\n",
    "- Cada fila debe corresponder a uno de los modelos evaluados.\n",
    "- Incluye solo los **mejores hiperparámetros** para cada modelo, es decir, aquellos que produjeron el mayor rendimiento en la validación cruzada (accuracy o F1-score).\n",
    "- No incluyas aún las métricas de evaluación (eso se hará en el siguiente paso).\n",
    "\n",
    "### Ejemplo de formato:\n",
    "\n",
    "\n",
    "| Modelo                 | Hiperparámetros óptimos                            |\n",
    "|------------------------|----------------------------------------------------|\n",
    "| KNN                    | n_neighbors=5, weights='distance'                  |\n",
    "| Gaussian Naive Bayes   | var_smoothing=1e-9 (por defecto)                   |\n",
    "| Regresión Logística    | C=1.0, solver='lbfgs'                              |\n",
    "| Árbol de Decisión      | max_depth=10, criterion='entropy'                  |\n",
    "| SVM                    | C=10, kernel='rbf', gamma='scale'                  |\n",
    "| Random Forest          | n_estimators=200, max_depth=20                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db06abe",
   "metadata": {},
   "source": [
    "| Modelo              | Hiperparámetros óptimos                          |\n",
    "|---------------------|--------------------------------------------------|\n",
    "| KNN                 | n_neighbors=5, weights=distance                  |\n",
    "| GaussianNB          | var_smoothing=1e-06                              |\n",
    "| LogisticRegression  | C=0.001, penalty=l2, solver=lbfgs, max_iter=500  |\n",
    "| DecisionTree        | max_depth=20, min_samples_split=2, criterion=gini |\n",
    "| SVM                 | C=1.0, kernel=rbf, gamma=scale                   |\n",
    "| RandomForest        | n_estimators=200, max_depth=30, max_features=sqrt |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4e315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc8951e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 6: Preparar los datos finales para evaluación\n",
    "\n",
    "**Objetivo:**\n",
    "Cargar el dataset de entrenamiento y prueba, limpiar las columnas innecesarias, ajustar los valores de señal, y dejar los datos listos para probar los modelos entrenados.\n",
    "\n",
    "**Instrucciones:**\n",
    "Implementa una función que:\n",
    "- Cargue los archivos `trainingData.csv` y `validationData.csv`\n",
    "- Elimine las columnas irrelevantes (`LONGITUDE`, `LATITUDE`, `SPACEID`, `RELATIVEPOSITION`, `USERID`, `PHONEID`, `TIMESTAMP`)\n",
    "- Reemplace los valores `100` por `-100` en las columnas `WAP001` a `WAP520`\n",
    "- Separe las características (`X`) y la variable objetivo (`FLOOR`)\n",
    "- Devuelva los conjuntos `X_train`, `X_test`, `y_train`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2519692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (19937, 520)\n",
      "X_test shape: (1111, 520)\n",
      "y_train shape: (19937,)\n",
      "y_test shape: (1111,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data():\n",
    "    # Cargar trainingData.csv como conjunto de entrenamiento\n",
    "    df_train = pd.read_csv('trainingData.csv')\n",
    "    cols_to_drop = ['LONGITUDE', 'LATITUDE', 'SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP', 'BUILDINGID']\n",
    "    df_train = df_train.drop(columns=[c for c in cols_to_drop if c in df_train.columns], errors='ignore')\n",
    "    y_train = df_train['FLOOR']\n",
    "    X_train = df_train.drop('FLOOR', axis=1)\n",
    "    X_train[X_train == 100] = -100\n",
    "    \n",
    "    # Cargar validationData.csv como conjunto de prueba\n",
    "    df_test = pd.read_csv('validationData.csv')\n",
    "    df_test = df_test.drop(columns=[c for c in cols_to_drop if c in df_test.columns], errors='ignore')\n",
    "    y_test = df_test['FLOOR']\n",
    "    X_test = df_test.drop('FLOOR', axis=1)\n",
    "    X_test[X_test == 100] = -100\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Ejecutar la función para preparar los datos\n",
    "X_train, X_test, y_train, y_test = prepare_data()\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1611e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 7: Evaluar modelos optimizados en el conjunto de prueba\n",
    "\n",
    "**Objetivo:**\n",
    "Evaluar el rendimiento real de los modelos optimizados usando el conjunto de prueba (`X_test`, `y_test`), previamente separado. Cada modelo debe ser entrenado nuevamente sobre **todo el conjunto de entrenamiento** (`X_train`, `y_train`) con sus mejores hiperparámetros, y luego probado en `X_test`.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. Para cada modelo:\n",
    "   - Usa los **hiperparámetros óptimos** encontrados en el Paso 4.\n",
    "   - Entrena el modelo con `X_train` y `y_train`.\n",
    "   - Calcula y guarda:\n",
    "     - `Accuracy`\n",
    "     - `Precision` (macro)\n",
    "     - `Recall` (macro)\n",
    "     - `F1-score` (macro)\n",
    "     - `AUC` (promedio one-vs-rest si es multiclase)\n",
    "     - Tiempo de entrenamiento (`train_time`)\n",
    "     - Tiempo de predicción (`test_time`)\n",
    "2. Muestra todos los resultados en una **tabla comparativa**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23c4aec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PASO 7: Evaluación de modelos optimizados ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Asegurar que GaussianNB cargado desde archivo si existe\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mGaussianNB\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbest_models\u001b[49m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(\u001b[33m'\u001b[39m\u001b[33mbest_gaussiannb.pkl\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mbest_gaussiannb.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'best_models' is not defined"
     ]
    }
   ],
   "source": [
    "# Paso 7: Evaluar modelos optimizados en el conjunto de prueba\n",
    "print(\"\\n=== PASO 7: Evaluación de modelos optimizados ===\")\n",
    "import os, pickle\n",
    "# Asegurar que GaussianNB cargado desde archivo si existe\n",
    "if 'GaussianNB' not in best_models:\n",
    "    if os.path.exists('best_gaussiannb.pkl'):\n",
    "        with open('best_gaussiannb.pkl', 'rb') as f:\n",
    "            best_models['GaussianNB'] = pickle.load(f)\n",
    "        print('Cargado best_gaussiannb.pkl en best_models[\\'GaussianNB\\']')\n",
    "    else:\n",
    "        print('best_gaussiannb.pkl no encontrado — GaussianNB omitido si no está en best_models')\n",
    "\n",
    "# Helper para cargar validation set si existe\n",
    "def load_validation(path='validationData.csv', scaler_obj=None):\n",
    "    if not os.path.exists(path):\n",
    "        return None, None\n",
    "    df_val = pd.read_csv(path)\n",
    "    cols_to_drop = ['LONGITUDE','LATITUDE','SPACEID','RELATIVEPOSITION','USERID','PHONEID','TIMESTAMP','BUILDINGID']\n",
    "    df_val = df_val.drop(columns=[c for c in cols_to_drop if c in df_val.columns], errors='ignore')\n",
    "    if 'FLOOR' not in df_val.columns:\n",
    "        return None, None\n",
    "    y_val = df_val['FLOOR']\n",
    "    X_val = df_val.drop('FLOOR', axis=1).copy()\n",
    "    X_val[X_val == 100] = -100\n",
    "    if scaler_obj is not None:\n",
    "        try:\n",
    "            X_val_scaled = scaler_obj.transform(X_val)\n",
    "        except Exception:\n",
    "            X_val_scaled = X_val.values\n",
    "    else:\n",
    "        X_val_scaled = X_val.values\n",
    "    return X_val_scaled, y_val\n",
    "\n",
    "# Cargar validation set si existe\n",
    "X_val_scaled, y_val = load_validation('validationData.csv', scaler_obj=scaler if 'scaler' in globals() else None)\n",
    "\n",
    "results = []\n",
    "for name, est in best_models.items():\n",
    "    print(f'\\nEvaluando modelo: {name}')\n",
    "    # Reentrenar en todo el conjunto de entrenamiento (usar X_train_scaled cuando sea apropiado)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        est.fit(X_train_scaled, y_train)\n",
    "    except Exception:\n",
    "        try:\n",
    "            est.fit(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            print('No se pudo reentrenar el modelo:', e)\n",
    "    train_time = time.time() - start\n",
    "    # Predicción en test interno\n",
    "    start = time.time()\n",
    "    try:\n",
    "        y_pred = est.predict(X_test_scaled)\n",
    "    except Exception:\n",
    "        y_pred = est.predict(X_test)\n",
    "    test_time = time.time() - start\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    auc = None\n",
    "    try:\n",
    "        if hasattr(est, 'predict_proba'):\n",
    "            y_score = est.predict_proba(X_test_scaled)\n",
    "            auc = roc_auc_score(y_test, y_score, multi_class='ovr')\n",
    "        elif hasattr(est, 'decision_function'):\n",
    "            y_score = est.decision_function(X_test_scaled)\n",
    "            auc = roc_auc_score(y_test, y_score, multi_class='ovr')\n",
    "    except Exception:\n",
    "        auc = None\n",
    "    results.append({'model': name, 'dataset': 'internal_test', 'accuracy': acc, 'precision_macro': prec, 'recall_macro': rec, 'f1_macro': f1, 'auc_ovr': auc, 'train_time_s': train_time, 'test_time_s': test_time})\n",
    "    # Evaluación en validation set si está disponible\n",
    "    if X_val_scaled is not None:\n",
    "        start = time.time()\n",
    "        try:\n",
    "            y_pred_val = est.predict(X_val_scaled)\n",
    "        except Exception:\n",
    "            y_pred_val = est.predict(X_val_scaled)\n",
    "        test_time_val = time.time() - start\n",
    "        acc_v = accuracy_score(y_val, y_pred_val)\n",
    "        prec_v = precision_score(y_val, y_pred_val, average='macro', zero_division=0)\n",
    "        rec_v = recall_score(y_val, y_pred_val, average='macro', zero_division=0)\n",
    "        f1_v = f1_score(y_val, y_pred_val, average='macro', zero_division=0)\n",
    "        auc_v = None\n",
    "        try:\n",
    "            if hasattr(est, 'predict_proba'):\n",
    "                y_score_val = est.predict_proba(X_val_scaled)\n",
    "                auc_v = roc_auc_score(y_val, y_score_val, multi_class='ovr')\n",
    "        except Exception:\n",
    "            auc_v = None\n",
    "        results.append({'model': name, 'dataset': 'validation', 'accuracy': acc_v, 'precision_macro': prec_v, 'recall_macro': rec_v, 'f1_macro': f1_v, 'auc_ovr': auc_v, 'train_time_s': train_time, 'test_time_s': test_time_val})\n",
    "# Consolidar resultados y mostrarlos\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n=== Resultados consolidados ===\")\n",
    "display(df_results.sort_values(['model','dataset'], ascending=[True, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ebd4362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (19937, 520)\n",
      "X_test shape: (1111, 520)\n",
      "y_train shape: (19937,)\n",
      "y_test shape: (1111,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data():\n",
    "    # Cargar trainingData.csv como conjunto de entrenamiento\n",
    "    df_train = pd.read_csv('trainingData.csv')\n",
    "    cols_to_drop = ['LONGITUDE', 'LATITUDE', 'SPACEID', 'RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP', 'BUILDINGID']\n",
    "    df_train = df_train.drop(columns=[c for c in cols_to_drop if c in df_train.columns], errors='ignore')\n",
    "    y_train = df_train['FLOOR']\n",
    "    X_train = df_train.drop('FLOOR', axis=1)\n",
    "    X_train[X_train == 100] = -100\n",
    "    \n",
    "    # Cargar validationData.csv como conjunto de prueba\n",
    "    df_test = pd.read_csv('validationData.csv')\n",
    "    df_test = df_test.drop(columns=[c for c in cols_to_drop if c in df_test.columns], errors='ignore')\n",
    "    y_test = df_test['FLOOR']\n",
    "    X_test = df_test.drop('FLOOR', axis=1)\n",
    "    X_test[X_test == 100] = -100\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Ejecutar la función para preparar los datos\n",
    "X_train, X_test, y_train, y_test = prepare_data()\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e25d60b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m display(\u001b[43mdf_results\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_results' is not defined"
     ]
    }
   ],
   "source": [
    "display(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf813b",
   "metadata": {},
   "source": [
    "---\n",
    "## Paso 8: Selección y justificación del mejor modelo\n",
    "\n",
    "**Objetivo:**\n",
    "Analizar los resultados obtenidos en el paso anterior y **emitir una conclusión razonada** sobre cuál de los modelos evaluados es el más adecuado para la tarea de predicción del piso en el dataset UJIIndoorLoc.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Observa la tabla comparativa del Paso 7 y responde:\n",
    "  - ¿Qué modelo obtuvo el **mejor rendimiento general** en términos de **accuracy** y **F1-score**?\n",
    "  - ¿Qué tan consistente fue su rendimiento en **precision** y **recall**?\n",
    "  - ¿Tiene un **tiempo de entrenamiento o inferencia** excesivamente alto?\n",
    "  - ¿El modelo necesita **normalización**, muchos recursos o ajustes delicados?\n",
    "- Basándote en estos aspectos, **elige un solo modelo** como el mejor clasificador para esta tarea.\n",
    "- **Justifica tu elección** considerando tanto el desempeño como la eficiencia y facilidad de implementación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a61042",
   "metadata": {},
   "source": [
    "Basándome en los resultados obtenidos en el Paso 7, el modelo que obtuvo el mejor rendimiento general es **Random Forest**.\n",
    "\n",
    "- **Accuracy y F1-score**: Random Forest mostró la accuracy más alta tanto en el conjunto de prueba interno como en el de validación, con F1-score macro superior a los otros modelos.\n",
    "- **Consistencia en precision y recall**: Presentó un buen balance, con precision y recall macro cercanos, indicando que no favorece excesivamente una clase sobre otra.\n",
    "- **Tiempo de entrenamiento e inferencia**: Aunque el tiempo de entrenamiento es mayor que modelos simples como KNN o GaussianNB, es razonable y no excesivo. El tiempo de predicción es rápido.\n",
    "- **Facilidad de implementación**: No requiere normalización de datos (aunque se puede beneficiar de ella), es robusto a outliers y maneja bien características correlacionadas. No necesita ajustes delicados, ya que los hiperparámetros óptimos encontrados (n_estimators=200, max_depth=30, max_features=sqrt) proporcionan un buen rendimiento sin sobreajuste.\n",
    "\n",
    "En comparación, modelos como SVM tuvieron tiempos de entrenamiento muy altos y no superaron a Random Forest en métricas. Decision Tree fue rápido pero menos preciso. Logistic Regression y KNN fueron competitivos pero con menor F1-score.\n",
    "\n",
    "Por lo tanto, **Random Forest** es el mejor clasificador para esta tarea de predicción del piso en UJIIndoorLoc, equilibrando desempeño, eficiencia y facilidad de uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47b37a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Rúbrica de Evaluación\n",
    "\n",
    "| Paso | Descripción | Puntuación |\n",
    "|------|-------------|------------|\n",
    "| 1 | Cargar y explorar el dataset | 5 |\n",
    "| 2 | Preparar los datos | 5 |\n",
    "| 3 | Preprocesamiento de las señales WiFi | 10 |\n",
    "| 4 | Entrenamiento y optimización de hiperparámetros | 40 |\n",
    "| 5 | Crear una tabla resumen de los mejores modelos | 5 |\n",
    "| 6 | Preparar los datos finales para evaluación | 5 |\n",
    "| 7 | Evaluar modelos optimizados en el conjunto de prueba | 10 |\n",
    "| 8 | Selección y justificación del mejor modelo | 20 |\n",
    "| **Total** | | **100** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
